# -*- coding: utf-8 -*-
"""Untitled142.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a1OdKFYQ9P2Dyh0BaukA3EJyqheKbCfr
"""

######### Final Code ##############
#!pip install PyPDF2
#!pip install -qqq groq==0.13.0 --progress-bar off
#!pip install markdown2
#!pip install reportlab
import os
from groq import Groq
import pandas as pd
import PyPDF2
import markdown2
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas

def pdf_readers(pdf_path):
  doc_files = []
  for i in pdf_path:
    doc = ''
    with open(i, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text = page.extract_text()
            doc+=text
            print(text)
    doc_files.append(doc)
  return doc_files



def generate_prompt(docs):
  prompts=[]
  for doc in docs:
    prompt = f'''
    Role:
    You are an evaluator tasked with assessing a Data Science workflow report generated by an LLM. The report aims to guide beginner data scientists through the end-to-end workflow for handling data.

    Task:
    Evaluate the report under "Document:" using the criteria outlined in "Evaluation Criteria:". Your evaluation must:
    1. **Provide a detailed explanation** in "Model Response."
    2. **Score the response (1-10)** based on its alignment with the "Gold Standard Answer."
    3. **Justify the score** in the "Reason:" section.

    Scoring Guidelines:
    - **1-3**: Response is mostly incorrect or missing key requirements.
    - **4-6**: Response contains some relevant points but is incomplete or unclear.
    - **7-9**: Response mostly aligns with the gold standard but lacks depth or precision.
    - **10**: Response fully meets all requirements with high accuracy and clarity.

    ---

    ### **Document:**
    {doc}

    ---

    ### **Evaluation Criteria:**
    The workflow report should address key aspects of data science in a structured, beginner-friendly manner. Your evaluation must check whether the report:
    - Clearly defines its **objective**.
    - Covers **data cleaning, preprocessing, and exploratory analysis**.
    - Explains **feature engineering, model selection, and training**.
    - Describes **evaluation metrics, validation, and deployment**.
    - Presents a **cohesive and logically structured** workflow.

    ---

    ### **Report Evaluation:**

    #### **1. What is the primary objective of this data science project, and has it been explicitly mentioned?**
    **Gold Standard Answer:**
    The reportâ€™s objective is to guide beginners through a structured workflow, covering data cleaning, visualization, and analysis, and helping users interpret results and derive actionable insights. This objective should be explicitly stated at the beginning of the report.

    *Model Response:*
    [Generated Answer Here]

    *Score:*
    [1-10]


    *Reason:*
    [Explanation of Score]

    ---

    #### **2. What are the essential steps in data cleaning and preprocessing?**
    *Gold Standard Answer:*
    A well-defined data cleaning section should include:
    - Removing duplicates and handling missing values.
    - Correcting errors and inconsistencies.
    - Converting data types (e.g., strings to dates).
    - Normalizing or standardizing numeric data.
    - Encoding categorical variables.

    *Model Response:*
    [Generated Answer Here]

    *Score:*
    [1-10]


    *Reason:*
    [Explanation of Score]

    ---

    #### **3. How should missing data be addressed?**
    **Gold Standard Answer:**
    The report should discuss analyzing missingness patterns and applying imputation techniques (mean, median, regression, KNN) or removing excessive missing values if necessary.

    **Model Response:**
    [Generated Answer Here]

    **Score:**
    [1-10]


    **Reason:**
    [Explanation of Score]

    ---

    #### **4. How are outliers detected and managed?**
    **Gold Standard Answer:**
    The report should explain outlier detection using statistical methods (z-scores, IQR) and visualizations (box plots, scatter plots) and provide options for removal, capping, or transformation.

    **Model Response:**
    [Generated Answer Here]

    **Score:**
    [1-10]

    **Reason:**
    [Explanation of Score]

    ---

    #### **5. What exploratory data analysis (EDA) techniques should be applied?**
    **Gold Standard Answer:**
    EDA should include summary statistics, visualizations (histograms, scatter plots, box plots), correlation analysis, and advanced plots (heatmaps, pair plots) for pattern detection.

    **Model Response:**
    [Generated Answer Here]

    **Score:**
    [1-10]

    **Reason:**
    [Explanation of Score]

    ---

    #### **6. How should feature engineering be approached?**
    **Gold Standard Answer:**
    Feature engineering should involve creating new features, scaling, encoding categorical variables, and applying dimensionality reduction techniques (PCA).

    **Model Response:**
    [Generated Answer Here]

    **Score:**
    [1-10]

    **Reason:**
    [Explanation of Score]

    ---

    #### **7. What criteria guide the selection of appropriate machine learning models?**
    **Gold Standard Answer:**
    Model selection depends on task type (classification, regression, clustering), data volume, feature characteristics, and interpretability needs. The report should recommend starting with simpler models (linear regression, decision trees) before progressing to complex ones (ensembles, neural networks).

    **Model Response:**
    [Generated Answer Here]

    **Score:**
    [1-10]

    **Reason:**
    [Explanation of Score]

    ---

    #### **8. How is model training conducted effectively?**
    **Gold Standard Answer:**
    The report should describe splitting data (training, validation, test), applying cross-validation, tuning hyperparameters, and using regularization techniques (Lasso, Ridge).

    **Model Response:**
    [Generated Answer Here]

    **Score:**
    [1-10]

    **Reason:**
    [Explanation of Score]

    ---

    #### **9. Which evaluation metrics should be used to assess model performance?**
    **Gold Standard Answer:**
    The choice of metrics depends on the problem:
    - Classification: Accuracy, precision, recall, F1-score, ROC-AUC.
    - Regression: RMSE, MAE, R-squared.
    The report should explain these metrics and their significance.

    **Model Response:**
    [Generated Answer Here]

    **Score:**
    [1-10]

    **Reason:**
    [Explanation of Score]

    ---

    #### **10. How is model performance validated and compared?**
    **Gold Standard Answer:**
    Performance validation should include cross-validation, holdout validation, and test dataset evaluation. The report should discuss model comparison techniques, including confusion matrices for classification tasks.

    **Model Response:**
    [Generated Answer Here]

    **Score:**
    [1-10]

    **Reason:**
    [Explanation of Score]

    ---

    #### **11. What steps are involved in deploying the model into production?**
    **Gold Standard Answer:**
    The report should cover deployment strategies, including API-based model serving, containerization (Docker), and CI/CD pipelines, ensuring real-time data handling and scalability.

    **Model Response:**
    [Generated Answer Here]

    **Score:**
    [1-10]


    **Reason:**
    [Explanation of Score]

    ---

    #### **12. What ethical and legal considerations must be taken into account?**
    **Gold Standard Answer:**
    Ethical considerations include data privacy, bias mitigation, and transparency. The report should also mention compliance with legal frameworks like GDPR or CCPA.

    **Model Response:**
    [Generated Answer Here]

    **Score:**
    [1-10]


    **Reason:**
    [Explanation of Score]

    ---

    #### **13. Does the report present a unified and logically structured overview of the data science workflow that is easy for a beginner to understand and offers clear, actionable insights?**
    **Gold Standard Answer:**
    A well-structured report should seamlessly connect all workflow stages, use accessible language, provide examples, and include visual aids.

    **Model Response:**
    [Generated Answer Here]

    **Score:**
    [1-10]

    **Reason:**
    [Explanation of Score]

    ####14. What is the average score of the report?
    *Overall Score:*
    [Average of all the above scores]


    *Reason:*
    [Summary of all the reasons mentioned above]

    ---

    ### **Final Instructions:**
    - Ensure your responses are **objective and detailed**.
    - If a model response **partially meets** the criteria, assign a **mid-range score (4-7)** instead of binary scoring.
    - If the report **misses key aspects**, explicitly mention **which requirements were not met** in the "Reason" section.

    '''
    prompts.append(prompt)

    return prompts



def main():
  #Define groq model for evaluation
  Groq_API_key= [API_KEY]
  client= Groq(api_key=Groq_API_key)

  #Generate prompt
  prompts = generate_prompt(doc_files)
  for i,prompt in enumerate(prompts):
    chat_completion = client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model="llama3-70b-8192"
            )
    llm_output = chat_completion.choices[0].message.content

    #print(f"Document {i+1} evaluation report:\n",llm_output)
    markdown_filename = "evaluation_report.md"
    with open(markdown_filename, "w", encoding="utf-8") as md_file:
        md_file.write(llm_output)

    # Convert Markdown to HTML
    html_content = markdown2.markdown(llm_output)

    html_filename = f"evaluation_report_{i+1}.html"
    with open(html_filename, "w", encoding="utf-8") as html_file:
        html_file.write(html_content)

    print(f"HTML file saved as: {html_filename}")
    print(f"Markdown saved as: {markdown_filename}")

if __name__ == "__main__":
    main()
