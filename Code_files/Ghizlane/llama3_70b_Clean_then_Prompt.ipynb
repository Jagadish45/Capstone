{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "g8eyhnN6xKnZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the groq package for querying the data\n",
    "%pip install -qqq groq==0.13.0\n",
    "# Install the fpdf package for creating the PDF report\n",
    "%pip install fpdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Groq and Pandas packages\n",
    "import os\n",
    "import httpx\n",
    "from groq import Groq\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all constants to be used\n",
    "GROQ_API_KEY = \"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\"\n",
    "\n",
    "# Define the dataset name\n",
    "MISSING_VALUE_INDICATORS = [\n",
    "    \"?\", \"NA\", \"N/A\", \"NaN\", \"NULL\", \"\", \"Unknown\", \"unknown\", \"missing\", \"Missing\"\n",
    "]\n",
    "\n",
    "# Define the model name to be used in chat completions\n",
    "LLM_MODEL_NAME = \"llama3-70b-8192\"\n",
    "\n",
    "# We can define a threshold for when to drop the values and when not \n",
    "#   For example: 20% is too much of a loss, so we can suggest replacing with the mean\n",
    "#   Anything between 0 and 5% is not too much of a loss and can be dropped.\n",
    "PREPROCESSING_THRES_HIGH_MISS = 20  # Replace by mean if >= 20% missing values\n",
    "PREPROCESSING_THRES_LOW_VAR = 5 # Drop if less or eq 5% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a groq client\n",
    "grop_client = Groq(\n",
    "    api_key=GROQ_API_KEY, \n",
    "    http_client=httpx.Client(verify=False)  # Disable SSL verification\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Functions\n",
    "\n",
    "### Introduction\n",
    "This section introduces all functions that should be distributed throughout our codebase for ease of use and readability.\n",
    "\n",
    "### Functions\n",
    "1. `load_dataset_from_file`: Load a dataset from a file.\n",
    "2. `generate_prompt_with_dataset`: Generate a prompt with a dataset.\n",
    "3. `preprocess_data`: Preprocess data.\n",
    "4. `create_statistical_summary`: Create a statistical summary.\n",
    "5. `eda`: Perform exploratory data analysis.\n",
    "6. `ml_suggestions`: Provide machine learning suggestions.\n",
    "7. `feature_egr`: Perform feature engineering.\n",
    "8. `model_deployment`: Analyze possible model deployments for data.\n",
    "9. `conclusion`: Provide a conclusion.\n",
    "\n",
    "10. `generate_report`: Generate a report by calling all the aforementioned functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_file(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a dataset from a file and return a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The path to the file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The loaded dataset as a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path, na_values=MISSING_VALUE_INDICATORS)\n",
    "        elif file_path.endswith('.xlsx'):\n",
    "            df = pd.read_excel(file_path, na_values=MISSING_VALUE_INDICATORS)\n",
    "        elif file_path.endswith('.json'):\n",
    "            df = pd.read_json(file_path)\n",
    "            df.replace(MISSING_VALUE_INDICATORS, pd.NA, inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Unsupported file format. Please upload a CSV, Excel, or JSON file.\")\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"The dataset is empty.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_with_dataset(df: pd.DataFrame, goal: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a prompt for the user to complete based on the dataset provided.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset to be described.\n",
    "    - goal (str): The goal or task that the user should complete.\n",
    "\n",
    "    Returns:\n",
    "    - str: The prompt to be displayed\n",
    "    \"\"\"\n",
    "\n",
    "    # Extracting basic dataset info\n",
    "    dataset_describe = df.describe().to_string()\n",
    "    dataset_head = df.head().to_string()\n",
    "    missing_values = ((df.isnull().sum())[df.isnull().sum()!=0]).to_string()\n",
    "    column_types = df.dtypes.to_string()\n",
    "    \n",
    "    # Format the prompt\n",
    "    prompt = f\"\"\"\n",
    "      Dataset Overview:\n",
    "      ################\n",
    "      {dataset_head}\n",
    "      ################\n",
    "\n",
    "      Dataset Statistics Summary:\n",
    "      ################\n",
    "      {dataset_describe}\n",
    "      ################\n",
    "\n",
    "      Missing Values:\n",
    "      ################\n",
    "      {missing_values}\n",
    "      ################\n",
    "\n",
    "      Column Data Types:\n",
    "      ################\n",
    "      {column_types}\n",
    "      ################\n",
    "\n",
    "      Your Task:\n",
    "      ################\n",
    "      {goal}\n",
    "      ################\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = {\"role\": \"system\", \"content\": \"\"\"\n",
    "            - Make this section of the report as structured and concise as possible.\n",
    "            - Provide detailed reasoning for each suggestion, explaining why it is suitable for this specific dataset.\n",
    "            - Refer to the dataset's structure and target variable in your suggestions.\n",
    "            - Each main title should be formatted as a markdown headings appropriately.\n",
    "            - Give this section an appropriate and descriptive title.\n",
    "            - Do not summarize the dataset again unless it is about changing the dataformat to help run the EDA technique.\n",
    "            - Assume that there are complementary sections that were created before this prompt to help the reader understand the context of \n",
    "            the dataset and there is no need to reiterate the same information.      \n",
    "            - Have a consistent structure for the report: heading 1 first, then heading 2, then numbers 1,2,3, etc. then bullet points. \n",
    "                 \"\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess the data by handling missing values and removing duplicates.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    - str: The preprocessed data summary.\n",
    "    \"\"\"\n",
    "\n",
    "    #data_before_cleaning = df.head(3).to_markdown(index=False)\n",
    "    \n",
    "    # Add unique values for categorical columns\n",
    "    unique_values = df.nunique()\n",
    "\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_values = missing_values[missing_values > 0]\n",
    "    missing_values_pct = (missing_values / df.shape[0] * 100).round(2).astype(str) + \"%\"\n",
    "\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    \n",
    "    #- The goal is to generate a data cleaning and preprocessing summary report.\n",
    "    #- Provide an overview of the dataset before cleaning:\n",
    "    # + system_prompt[\"content\"]\n",
    "\n",
    "    data_preproc_clean = f\"\"\"\n",
    "    - You must first display the head of the dataset in your response in table format including all the rows.\n",
    "    - The dataset initially contains {df.shape[1]} columns and {df.shape[0]} rows.\n",
    "    - {len(missing_values)} columns with missing values such that {missing_values_pct}.\n",
    "    - {duplicate_count} duplicate rows found.\n",
    "    - Unique values for each column: {unique_values.to_string()}\n",
    "    - This section of the report is entitled Data Preprocessing & Cleaning.\n",
    "    - Compare the dataset before and after cleaning.\n",
    "    - Mention how the dataset was affected after cleaning.\n",
    "    - Make this section of the report as structured and concise as possible.\n",
    "    - Provide detailed reasoning for each suggestion, explaining why it is suitable for this specific dataset.\n",
    "    - Each main title should be formatted as a markdown headings appropriately.\n",
    "    - Have a consistent structure for the report: heading 1 first, then heading 2, then numbers 1,2,3, etc. then bullet points. \n",
    "\"\"\"\n",
    "\n",
    "    prompt = generate_prompt_with_dataset(df, goal=data_preproc_clean)\n",
    "\n",
    "    chat_completion = grop_client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=LLM_MODEL_NAME\n",
    "    )\n",
    "\n",
    "    summary = chat_completion.choices[0].message.content\n",
    "    #print(summary)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_statistical_summary(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate a summary of the statistical analysis of the dataset.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "    - str: The summary of the statistical analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    summary = \"\"\"## **Statistical Analysis**\\n\"\"\" + df.describe().to_markdown()\n",
    "\n",
    "    prompt = generate_prompt_with_dataset(\n",
    "        df, goal=f\"\"\"\n",
    "        - You must first display all the rows {df.describe().to_markdown()} in table format.\n",
    "        - Summarize the following findings under the Statistical Analysis section:\n",
    "        - Make this section of the report as structured and concise as possible.\n",
    "        - Provide detailed reasoning for each suggestion, explaining why it is suitable for this specific dataset.\n",
    "        - Each main title should be formatted as a markdown headings appropriately.\n",
    "        - Interpret the statistical analysis results and suggest next steps and say something about the variables distributions to help decide on what to do next using the {df.describe().to_markdown()}\n",
    "        - Have a consistent structure for the report: heading 1 first, then heading 2, then numbers 1,2,3, etc. then bullet points. \n",
    "        \"\"\")\n",
    "\n",
    "    chat_completion = grop_client.chat.completions.create(\n",
    "        messages=[system_prompt,\n",
    "            #{\"role\": \"system\", \"content\": \"Make sure you put the title of the section as a markdown heading. The title of this section is '# Statistical Analysis'.\"}, \n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        model=LLM_MODEL_NAME\n",
    "    )\n",
    "\n",
    "    summary = chat_completion.choices[0].message.content\n",
    "    #print(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate a summary of the Exploratory Data Analysis (EDA) suggestions for the dataset.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "    - str: The summary of the EDA suggestions.\n",
    "    \"\"\"\n",
    "    prompt = generate_prompt_with_dataset(df, goal=\"\"\"generate Exploratory Data Analysis\"\"\")\n",
    "    \n",
    "    chat_completion = grop_client.chat.completions.create(\n",
    "        messages=[system_prompt, {\"role\": \"user\", \"content\": prompt}],\n",
    "        model=LLM_MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    summary = chat_completion.choices[0].message.content\n",
    "    #print(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_suggestions(df) -> str:\n",
    "    \"\"\"\n",
    "    Generate a summary of the Machine Learning Algorithm Selection suggestions for the dataset.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset to be analyzed.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The summary of the Machine Learning Algorithm Selection suggestions.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = generate_prompt_with_dataset(df, goal=\"\"\"generate Supervised and Unsupervised Machine Learning suggestions based on the dataset provided.\"\"\")\n",
    "\n",
    "    chat_completion = grop_client.chat.completions.create(\n",
    "            messages=[system_prompt, {\"role\": \"user\", \"content\": prompt}],\n",
    "            model=LLM_MODEL_NAME\n",
    "        )\n",
    "\n",
    "    summary = chat_completion.choices[0].message.content\n",
    "    #print(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_egr(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate a summary of the Feature Engineering suggestions for the dataset.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "    - str: The summary of the Feature Engineering suggestions.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = generate_prompt_with_dataset(df, goal=\"\"\"generate Feature Engineering suggestions based on the dataset provided.\"\"\")\n",
    "\n",
    "    chat_completion = grop_client.chat.completions.create(\n",
    "        messages=[system_prompt, {\"role\": \"user\", \"content\": prompt}],\n",
    "        model=LLM_MODEL_NAME\n",
    "    )\n",
    "\n",
    "    summary = chat_completion.choices[0].message.content\n",
    "    #print(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_deployment(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate a summary of the Model Deployment Strategies and Handling Data Drift suggestions for the dataset.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "    - str: The summary of the Model Deployment Strategies and Handling Data Drift suggestions.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = generate_prompt_with_dataset(df, goal =  \"\"\"generate Model Deployment Strategies and Handling Data Drift suggestions based on the dataset provided.\"\"\")\n",
    "    \n",
    "    chat_completion = grop_client.chat.completions.create(\n",
    "            messages=[system_prompt, {\"role\": \"user\", \"content\": prompt}],\n",
    "            model=LLM_MODEL_NAME\n",
    "        )\n",
    "    \n",
    "    summary = chat_completion.choices[0].message.content\n",
    "    #print(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclusion(df: pd.DataFrame, report: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a conclusion for the data science workflow report.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset used in the report.\n",
    "    - report (str): The report content.\n",
    "\n",
    "    Returns:\n",
    "    - str: The conclusion of the data science workflow report.\n",
    "    \"\"\"\n",
    "\n",
    "    data_conclusion = generate_prompt_with_dataset(df, goal =  f\"\"\"\n",
    "        - This is the last section of the report and the goal from this part of the report is to generate a conclusion based on the dataset provided and the data report file.\n",
    "        - Here is the report: \n",
    "            ############\n",
    "            {report}\n",
    "            ############\n",
    "        - Make it as structured and concise as possible.\n",
    "        - Give this section an appropriate and descriptive title.\n",
    "        - Consider that there are complementary sections that were created before this prompt to help the reader understand the context of the dataset and there is no need to reiterate the same information.\n",
    "        - Name this section Conclusion.\n",
    "    \"\"\")\n",
    "\n",
    "    chat_completion = grop_client.chat.completions.create(\n",
    "            messages=[system_prompt, {\"role\": \"user\", \"content\": data_conclusion}],\n",
    "            model=LLM_MODEL_NAME\n",
    "        )\n",
    "    \n",
    "    summary = chat_completion.choices[0].message.content\n",
    "    #print(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(df: pd.DataFrame, file_name: str = \"automated_report.md\") -> str:\n",
    "    \"\"\"\n",
    "    Generate an AI report for a given dataset and write it to a markdown file.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The input dataset.\n",
    "\n",
    "    Returns:\n",
    "    str: The absolute path of the generated markdown file.\n",
    "    \"\"\"\n",
    "\n",
    "    functions = [preprocess_data, create_statistical_summary, eda, ml_suggestions, feature_egr, model_deployment]\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(lambda func: func(df), functions))\n",
    "\n",
    "    report = \"\\n\\n\".join(results)\n",
    "    report += \"\\n\\n\"+conclusion(df, report)\n",
    "\n",
    "    # Write the report to a file\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    # Return file's absolute path\n",
    "    return os.path.abspath(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ghizlanerehioui/Downloads/Capstone/Code_files/Ghizlane/Auto.md'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"../../Data Files/Automobile/Automobile_data.csv\"\n",
    "df = load_dataset_from_file(file_path)\n",
    "generate_report(df, \"Auto.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ghizlanerehioui/Downloads/Capstone/Code_files/Ghizlane/Reviews.md'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"../../Data Files/Customer Experience/Car_Reviews_Database.csv\"\n",
    "df = load_dataset_from_file(file_path)\n",
    "df.head()\n",
    "generate_report(df, \"Reviews.md\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
