{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqqq pip --progress-bar off\n",
        "!pip install -qqq groq==0.13.0 --progress-bar off\n",
        "!pip install -qqq python-dotenv==1.0.1 --progress-bar off"
      ],
      "metadata": {
        "id": "g8eyhnN6xKnZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzP3qle3F5Ug",
        "outputId": "d3628878-a7a9-4243-f497-202a005cc815"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "TDMVjqSo7r1Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Groq_API_key= \"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\"\n",
        "client= Groq(api_key=Groq_API_key)\n"
      ],
      "metadata": {
        "id": "LR3iS1bXxMI2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MISSING_VALUE_INDICATORS = [\n",
        "    \"?\", \"NA\", \"N/A\", \"NaN\", \"NULL\", \"\", \"Unknown\", \"unknown\", \"missing\", \"Missing\"\n",
        "]\n",
        "\n",
        "\n",
        "# Function to load dataset with flexible missing value handling\n",
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        if file_path.endswith('.csv'):\n",
        "            df = pd.read_csv(file_path, na_values=MISSING_VALUE_INDICATORS)\n",
        "        elif file_path.endswith('.xlsx'):\n",
        "            df = pd.read_excel(file_path, na_values=MISSING_VALUE_INDICATORS)\n",
        "        elif file_path.endswith('.json'):\n",
        "            df = pd.read_json(file_path)\n",
        "            df.replace(MISSING_VALUE_INDICATORS, pd.NA, inplace=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported file format. Please upload a CSV, Excel, or JSON file.\")\n",
        "\n",
        "        if df.empty:\n",
        "            raise ValueError(\"The dataset is empty.\")\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "t4FEhQT5tswU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt_with_dataset(df):\n",
        "    # Basic dataset info\n",
        "    dataset_info = df.describe().to_string()\n",
        "    dataset_head = df.head().to_string()\n",
        "    missing_values = df.isnull().sum().to_string()\n",
        "    column_types = df.dtypes.to_string()\n",
        "\n",
        "    '''print(\"Dataset Info:\")\n",
        "    print(dataset_info)\n",
        "\n",
        "    print(\"\\nDataset Head:\")\n",
        "    print(dataset_head)\n",
        "\n",
        "    print(\"\\nMissing Values:\")\n",
        "    print(missing_values)\n",
        "\n",
        "    print(\"\\nColumn Data Types:\")\n",
        "    print(column_types)'''\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "  Given the following dataset, describe the entire data science workflow that can be applied to it. Provide data-specific suggestions** with detailed reasoning for each step. Cover the following aspects in detail:\n",
        "\n",
        "  Dataset Overview:\n",
        "  {dataset_head}\n",
        "\n",
        "  Dataset Summary:\n",
        "  {dataset_info}\n",
        "\n",
        "  Missing Values:\n",
        "  {missing_values}\n",
        "\n",
        "  Column Data Types:\n",
        "  {column_types}\n",
        "\n",
        "  1. Data Preprocessing & Cleaning\n",
        "      - Identify specific columns with missing values and suggest appropriate handling techniques for each. For example:\n",
        "          - For numeric columns (e.g., \"normalized-losses\"), suggest techniques like mean/median imputation or dropping rows, and explain why the chosen technique is suitable.\n",
        "          - For categorical columns (e.g., \"num-of-doors\"), suggest techniques like mode imputation or creating a \"missing\" category, and explain why the chosen technique is suitable.\n",
        "      - Identify and handle duplicate records, if any.\n",
        "      - Suggest techniques for handling inconsistent data (e.g., standardizing units, correcting typos).\n",
        "\n",
        "  2. Exploratory Data Analysis (EDA)\n",
        "      - For each column, suggest specific EDA techniques and visualizations. For example:\n",
        "          - For numeric columns (e.g., \"horsepower\"), suggest histograms, box plots, or scatter plots, and explain why these are suitable.\n",
        "          - For categorical columns (e.g., \"fuel-type\"), suggest bar charts, count plots, or pie charts, and explain why these are suitable.\n",
        "      - Identify relationships between columns and suggest techniques like correlation matrices or pair plots.\n",
        "      - Suggest dimensionality reduction techniques (e.g., PCA) if applicable, and explain why.\n",
        "\n",
        "  3. Machine Learning Algorithm Selection\n",
        "      - Based on the dataset's structure and target variable (if any), suggest specific machine learning algorithms. there could be multiple target variables and multiple ways to use Machine learning on this dataset. For example:\n",
        "          - If the task is classification (e.g., predicting \"fuel-type\"), suggest algorithms like logistic regression, decision trees, or random forests, and explain why they are suitable.\n",
        "          - If the task is regression (e.g., predicting \"price\"), suggest algorithms like linear regression, decision trees, or gradient boosting, and explain why they are suitable.\n",
        "      - Compare supervised vs. unsupervised approaches and suggest which is more appropriate for this dataset.\n",
        "      - Discuss model evaluation metrics (e.g., accuracy, precision, recall, RMSE, F1-score) and explain which metrics are most relevant for this dataset.\n",
        "\n",
        "  4. Model Optimization & Feature Engineering\n",
        "      - Suggest specific feature engineering techniques for this dataset. For example:\n",
        "          - For numeric columns, suggest applicable techniques for example, normalization, standardization, or log transformation, and explain why.\n",
        "          - For categorical columns, suggest techniques like one-hot encoding, label encoding, or target encoding, and explain why.\n",
        "      - Suggest hyperparameter tuning techniques (e.g., GridSearchCV, RandomizedSearchCV) and explain how they can improve model performance.\n",
        "      - Discuss the benefits of ensemble learning (e.g., bagging, boosting, stacking) for this dataset.\n",
        "\n",
        "  5. Deployment & Real-World Considerations\n",
        "      - Suggest how models trained on this dataset can be deployed in real-world applications (e.g., APIs, cloud services, edge devices).\n",
        "      - Discuss strategies for handling data drift, model monitoring, and retraining to maintain model performance over time.\n",
        "\n",
        "  Provide detailed reasoning for each suggestion, explaining why it is suitable for this specific dataset. Use examples from the dataset (e.g., column names, data types) to make your recommendations as specific as possible.\n",
        "  \"\"\"\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "zoOKdxeO73IP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "import re\n",
        "\n",
        "class PDF(FPDF):\n",
        "    def header(self):\n",
        "        \"\"\"Set the title in the header of the PDF\"\"\"\n",
        "        self.set_font(\"Arial\", \"B\", 14)\n",
        "        self.cell(200, 10, \"Data Science Workflow Report\", ln=True, align=\"C\")\n",
        "        self.ln(10)\n",
        "\n",
        "    def footer(self):\n",
        "        \"\"\"Page footer with page number\"\"\"\n",
        "        self.set_y(-15)\n",
        "        self.set_font(\"Arial\", \"I\", 10)\n",
        "        self.cell(0, 10, f\"Page {self.page_no()}\", align=\"C\")\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Replace Unicode characters and remove Markdown-style formatting\"\"\"\n",
        "    replacements = {\n",
        "        \"•\": \"-\",  # Bullet points\n",
        "        \"–\": \"-\",  # En dash\n",
        "        \"—\": \"-\",  # Em dash\n",
        "        \"“\": '\"',  # Left quote\n",
        "        \"”\": '\"',  # Right quote\n",
        "        \"‘\": \"'\",  # Left single quote\n",
        "        \"’\": \"'\",  # Right single quote\n",
        "    }\n",
        "    for key, value in replacements.items():\n",
        "        text = text.replace(key, value)\n",
        "\n",
        "    text = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def markdown_to_pdf(text, output_file):\n",
        "    pdf = PDF()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "    pdf.add_page()\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "    text = clean_text(text)\n",
        "    lines = text.split(\"\\n\")\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        if not line:\n",
        "            pdf.ln(5)\n",
        "            continue\n",
        "\n",
        "        # formatting section headings\n",
        "        if re.match(r\"^[A-Z][a-zA-Z\\s]+$\", line):\n",
        "            pdf.set_font(\"Arial\", \"B\", 12)\n",
        "            pdf.cell(0, 10, line, ln=True)\n",
        "            pdf.set_font(\"Arial\", size=12)\n",
        "            pdf.ln(2)\n",
        "            continue\n",
        "\n",
        "        # Detecting bullet points (`* ` or `- `) and formatting properly\n",
        "        bullet_match = re.match(r\"^(\\*|-)\\s+(.*)\", line)\n",
        "        if bullet_match:\n",
        "            pdf.cell(5)  # Indentation for bullets\n",
        "            pdf.set_font(\"Arial\", \"B\", 12)  # Bold for bullet label\n",
        "            pdf.cell(5, 10, \"-\", ln=False)  # Use ASCII-compatible bullet\n",
        "            pdf.set_font(\"Arial\", size=12)  # Normal text for content\n",
        "            pdf.multi_cell(0, 8, f\" {bullet_match.group(2)}\")\n",
        "            continue\n",
        "\n",
        "        pdf.multi_cell(0, 8, line)\n",
        "\n",
        "    # Save the PDF\n",
        "    pdf.output(output_file, \"F\")\n",
        "    print(f\"PDF saved as {output_file}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dfNhdOZcFZxq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "\n",
        "def main():\n",
        "    file_path = \"Automobile_data.csv\"\n",
        "    df = load_dataset(file_path)\n",
        "\n",
        "    if df is not None:\n",
        "        # Generating the prompt\n",
        "        prompt = generate_prompt_with_dataset(df)\n",
        "\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            model=\"llama3-70b-8192\"\n",
        "        )\n",
        "\n",
        "        llm_output = chat_completion.choices[0].message.content\n",
        "\n",
        "        #saving the response to pdf\n",
        "        markdown_to_pdf(llm_output, \"formatted_llm_output.pdf\")\n",
        "\n",
        "# main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG-Iwkv97_V7",
        "outputId": "d2ba4c3e-19a1-4bc9-f3a9-6a93dbf4a074"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF saved as formatted_llm_output.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bdAmXkwTsOD"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}